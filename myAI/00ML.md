# 0 Machine Learning --- Andrew Ng

## 3

### 3.1 Matrix and Vector

- Matrix: Rectangular array of numbers
  - Dimension of matrix: number of rows X number of columns
  - Matrix Elements: entries of matrix
  - $A_{ij}$ = "$i$,$j$ entry" in the $i$th row, $j$th column
- Vector: An n X 1 matrix
  - $y_i$ = $i$th element

### 3.2 Matrix Addition and Scalar Multiplication

### 3.3 Matrix-vector multiplication

### 3.4 Matrix-matrix multiplication

### 3.5 Matrix multiplication properties

- let A and B be matrices. Then in general, $A \times B \neq B \times A$
- $Z = A \times B \times C$
  - let $D = A \times B$, then $Z = D \times C$
- Identity Matrix
  - Denoted $I$(or $I_{n \times n}$)
  - For any matrix A, $A \times I = I \times A = A$

### 3.6 Inverse and Transpose

- Inverse
  - Not all numbers have an inverse
  - If A is an mxm matrix, and if it has an inverse, $A\left(A^{-1}\right) = \left(A^{-1}\right)A = I$
  - matrix that don't have an inverse are "singular" or "degenerate"
- Transpose
  - Let  A be an mxn matrix and let $B = A_T$, then B is an nxm matrix and $B_{ij} = A_{ji}$

## 4 Linear Regression with multiple variables

### 4.1 Multiple features

- Notation:
  - $n$ = number of features
  - $x_{(i)}$ = input(features) of $i^{th}$ training example
  - $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example
- Hypothesis:
  - Previously: $h_\theta(x) = \theta_0 + {\theta_1}x$
  - $h_\theta(x) = {\theta_0}{x_0} + {\theta_1}{x_1} + {\theta_2}{x_2} + {\theta_3}{x_3}$, for convenience of notation, define $x_0=1$

$$
X =
\begin{bmatrix}
    x_0\\
    x_1\\
    x_2\\
    \vdots\\
    x_n\\
\end{bmatrix}
\quad
\theta =
\begin{bmatrix}
    \theta_0\\
    \theta_1\\
    \theta_2\\
    \vdots\\
    \theta_n\\
\end{bmatrix}
\quad
h_\theta\left(x\right)=\theta^TX
$$

- Multivariate linear regression

### 4.2 Gradient descent for multiple variables

- Hypothesis: $h_\theta(x) = \theta^TX = {\theta_0}{x_0} + {\theta_1}{x_1} + {\theta_2}{x_2} + \cdots + {\theta_n}{x_n}$
- Parameters: $\theta_0, \theta_1, \theta_2, \cdots, \theta_n$
- Cost function: $J\left(\theta_0, \theta_1, \theta_2, \cdots, \theta_n\right) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$
- Gradient descent: Repeat{ $\theta_j\,:=\,\theta_j-\alpha\frac{\partial}{\partial\theta_j}J\left(\theta_0, \theta_1, \theta_2, \cdots, \theta_n\right) = \frac{1}{m}\sum\limits_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}$ } (Simultaneously update for every $j=\left(0,\cdots,n\right)$)

### 4.3 Gradient descent in practice: Feature Scaling

- Feature Scaling
  - Get every feature into approximately a  $-1 \le x_i \le 1$range
- Mean normalization
  - Replace $x_i$ with $x_i - \mu_i$ to make features have approximately zero mean

### 4.4

- if $\alpha$ is too small: slow convergence
- if $\alpha$ is too large: $J(\theta)$ may not decrease on every iteration; may not converge

### 4.5 Features and polynomial regression

### 4.6 Normal equation

- Normal equation: Method to solve for $\theta$ analytically
  - $\theta = \left(X^TX\right)^{-1}X^Ty$
  - No need to choose $\alpha$
  - Don't need to iterate
  - Need to compute $\left(X^TX\right)^{-1}$
  - Slow if $n$ is very large

### 4.7 Normal equation and non-invertibility

- What it $X^TX$ is non invertible?
  - Redundant features(linearly dependent)
  - Too many features(m < n)
    - Delete some features, or use regularization

## 5 Octave Tutorial

## 6 Logistic Regression

### 6.1 Classification

- $y \in \{0, 1\}$
  - 0: "Negative Class"
  - 1: "Positive Class"
- Logistic regression: $0 \le h_\theta(x) \le 1$

### 6.2 Hypothesis Representation

- Want: $0 \le h_\theta(x) \le 1$
  - $h_\theta(x) = g\left(\theta^Tx\right)$
  - $g(z) = \frac{1}{1+\rm e^{-z}}$
  - Sigmoid function \ Logistic function
- Interpretation of Hypothesis Output
  - $h_\theta(x)$ = estimated probability that y = 1 on input x

### 6.3 Decision boundary

- Non-linear decision boundaries

### 6.4 Cost function

- Logistic regression cost function
- $\text {Cost}\left(h_\theta(x), y\right) = \frac{1}{2}\left(h_\theta\left(x\right)-y\right)^2$

$$
\text {Cost}\left(h_\theta(x), y\right) =
\begin{cases}
  -\log\left(h_\theta(x)\right)\quad & \rm if \ y=1\\
  -\log\left(1-h_\theta(x)\right)\quad & \rm if \ y=0\\
\end{cases}
$$

### 6.5 Simplified cost function and gradient descent

- $\text {Cost}\left(h_\theta(x), y\right) = -y\log(h_\theta(x))-(1-y)\log(1-h_\theta(x))$

### 6.6 Advanced optimization

- Optimization algorithm
  - Cost function $J(\theta)$. Want $min_{\theta}J(\theta)$
  - Given $\theta$, we have code that can compute
    - $J(\theta)$
    - $\frac{\partial}{\partial\theta_j}J(\theta)$
  - Gradient descent: Repeat{ ${\theta}_j := {\theta}_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)$ }

### 6.7 Multi-class classification: One-VS-all

## 7 Regularization

### 7.1 The problem of overfitting

- Overfitting: High variance
  - If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples.
- Underfitting: High bias
- Addressing overfitting:
  1. Reduce number of fratures
     - Manually select which features to keep
     - Model selection algorithm
  2. Regularization
     - Keep all the features, but reduce magnitude/ value of parameters $\theta_j$
     - Works well when we have a lot of features, each of which contributes a bit to prediceting $y$

### 7.2 Cost function

- Regularization
  - Small value for parameters $\theta_0, \theta_1, \cdots, \theta_n$
    - "Simpler" hypothesis
    - Less prone to overfitting
  - $J\left(\theta\right) = \frac{1}{2m}\sum\limits_{i=1}^m\left[\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2+\lambda\sum\limits_{j=1}^n\theta_j^2\right]$
  - $min_\theta{J(\theta)}$

### 7.3 Regularized linear regression

- Non-invertibility
  - $\theta = \left(X^TX\right)^{-1}X^Ty$

### 7.4 Regularized logistic regression

## 8 Neural Networks: Representation

### 8.1 Non-linear hypotheses

### 8.2 Neurons and the brain

- Neural Networks
  - Origins: Algorithms that try to mimic the brain
  - Was very widely used in 80s and early 90s; ppularity diminished in late 90s
  - Recent resurgence: State-of-the-art technique for many applications

### 8.3 Model representation I

- $\alpha_i^{(j)}$ = "activation" of unit $i$ in layer $j$
- $\theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$

### 8.4 Model representation II

- Foward propagation: Vectorized implementation
- Neural Network learning its own features

### 8.5 Examples and intuitions I

- Non-linear classification example: XOR/XNOR

### 8.6 Examples and intuitions II

### 8.7 Multi-class classification

- Multiple output units: One-vs-all

## 9 Neural Networks: Learning

### 9.1 Cost Function

- Logistic regression
  - $$
- Neural network:
  - $J(\theta) = - \frac{1}{m}\left[\sum\limits_{i=1}^m\sum\limits_{k=1}^K y_k^{(i)}\log\left(h_\theta\left(x^{(i)}\right)\right)_k+\left(1-y_k^{(i)}\right)\log\left(1-\left(h_\theta\left(x^{(i)}\right)\right)_k\right)\right] + \frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_l+1}\left(\theta_{ji}^{(l)}\right)^2$

### 9.2 Backpropagation algorithm

- Gradient computation: Backpropagation algorithm
  - Intuition: $\delta_j^{(l)}$ = "error" of node $j$ in layer $l$

### 9.3 Backpropagation intuition

### 9.4 Implementation note: Unrolling parameters

### 9.5 Gradient checking

### 9.6 Random initialization

### 9.7 Putting it together

- Training a neural network
  - 1. Randomly initialize weights
  - 2. Implement forward propagation to get $h_\theta\left(x^{(i)}\right)$ for any $x^{(i)}$
  - 3. Implement code to compute cost function $J(\theta)$
  - 4. Implement backprop to compute partial derivatives $\frac{\partial}{\partial\theta_{jk}^{(i)}J(\theta)}$
  - 5. Use gradient checking to compare $\frac{\partial}{\partial\theta_{jk}^{(i)}J(\theta)}$ computed using backpropagation vs. using numerical estimate of gradient of $J(\theta)$
  - 6. Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\theta)$ as a function of parameters $\theta$

### 9.8 Autonomous driving example